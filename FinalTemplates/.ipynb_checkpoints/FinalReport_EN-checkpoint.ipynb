{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Detection From Chest X-Ray Images\n",
    "\n",
    "### Group Members\n",
    "\n",
    "#### Okan ALAN : 21526638\n",
    "#### Batuhan METE : 21627538\n",
    "\n",
    "![https://media.giphy.com/media/ihpWHmgndDX6Vmcnj0/giphy.gif](https://media.giphy.com/media/ihpWHmgndDX6Vmcnj0/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "[Problem](#problem)   \n",
    "[Data Understanding](#data_understanding)   \n",
    "[Data Preparation](#data_preparation)   \n",
    "[Modeling](#modeling)   \n",
    "[Evaluation](#evaluation)   \n",
    "[References](#references)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem <a class=\"anchor\" id=\"problem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we learned that the project theme is health, we couldn't choice the project topic. Then the COVID-19 virus situation evolved. We tought we can use covid-19 data into our project because it is a viral topic but we hearded some groups already sent their project ideas to you. When we research a dataset for COVID-19 we learned symptoms of pneumonia is same. After that, we changed the our topic. We select the pneumonia problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Pneumonia?\n",
    "\n",
    "Pneumonia is a lung infection that can range from mild to so severe that you have to go to the hospital. It happens when an infection causes the air sacs in your lungs (your doctor will call them alveoli) to fill with fluid or pus. That can make it hard for you to breathe in enough oxygen to reach your bloodstream. Anyone can get this lung infection. But infants younger than age 2 and people over age 65 are at higher risk. That’s because their immune systems might not be strong enough to fight it. Causes include bacteria, viruses, and fungi.  If your pneumonia results from bacteria or a virus, you can spread it to someone else. Lifestyle habits, like smoking cigarettes and drinking too much alcohol, can also raise your chances of getting pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symptoms of Pneumonia\n",
    "\n",
    "Symptoms can vary depending on what’s causing pneumonia, age, and overall health. They usually develop over several days.\n",
    "\n",
    "**Common pneumonia symptoms include:**\n",
    "\n",
    "- Chest pain when you breathe or cough\n",
    "- Cough that produces phlegm or mucus\n",
    "- Fatigue and loss of appetite\n",
    "- Fever, sweating, and chills\n",
    "- Nausea, vomiting, and diarrhea\n",
    "- Shortness of breath\n",
    "\n",
    "Along with these symptoms, older adults and people with weak immune systems might be confused or have changes in mental awareness, or they might have a lower-than-usual body temperature. Newborns and infants may not show any signs of infection. Or they might vomit, have a fever and a cough, and seem restless or tired.\n",
    "\n",
    "**Attention Please** If you have a new cough, fever, or shortness of breath, call your doctor to ask about whether it could be COVID-19. Illness with the new coronavirus can also lead to pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pneumonia is typically diagnosed based on a combination of physical signs and a chest X-ray. In adults with normal vital signs and a normal lung examination, the diagnosis is unlikely. However, the underlying cause can be difficult to confirm, as there is no definitive test able to distinguish between bacterial and non-bacterial origin. The overall impression of a physician appears to be at least as good as decision rules for making or excluding the diagnosis.\n",
    "\n",
    "A chest radiograph is frequently used in diagnosis. In people with mild disease, imaging is needed only in those with potential complications, those not having improved with treatment or those in which the cause is uncertain. If a person is sufficiently sick to require hospitalization, a chest radiograph is recommended. Findings do not always match the severity of disease and do not reliably separate between bacterial infection and viral infection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had two different types of datasets for this topic. The first one was consisting of yes/no answers to the above symptoms. Another one was the Chest X-Ray images. Instead of using the first way, we decided to use a labeled dataset that contains X-Ray images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding<a class=\"anchor\" id=\"data_understanding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification. There are 3 versions. We used version 2. This dataset is prepared by Universtiy of UC San Diego.\n",
    "\n",
    "**Description**\n",
    "\n",
    "Dataset of validated OCT and Chest X-Ray images described and analyzed in \"Deep learning-based classification and referral of treatable human diseases\". The OCT Images are split into a training set and a testing set of independent patients. OCT Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL. We used only X-Ray Images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content**\n",
    "\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,856 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n",
    "\n",
    "|            | Normal Count | Pneumonia Count | Total |\n",
    "|------------|--------------|-----------------|-------|\n",
    "| Train      | 1349         | 3883            | 5232  |\n",
    "| Test       | 234          | 390             | 624   |\n",
    "| Total      | 1583         | 4273            | 5856  |\n",
    "\n",
    "![](./sampleImages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Folder Outline**\n",
    "\n",
    "Dataset <br />\n",
    "&nbsp;&nbsp;|---->test <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|-----> NORMAL<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|----->PNEUMONIA<br>\n",
    "&nbsp;&nbsp;|---->train<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|-----> NORMAL<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|----->PNEUMONIA<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation<a class=\"anchor\" id=\"data_preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 3 different machine learning model in the project. These are K-Nearest-Neighbours(KNN), Convolutional Neural Network(CNN) that is created by us by trial and error and VGG16 that is wellknown CNN architecture wihich was used to win ImageNet competition in 2014. You can say there is 2 main model but we will explain why we said 3 in next section. We split our data set to train and test it before coding because we thought and researched which of the following ways was correct for comparable results.\n",
    "\n",
    "- Splitting dataset before do anything\n",
    "- Splitting dataset randomly for each model started\n",
    "\n",
    "We know the second option is not stable because trained images change for each running but we used 3 different model and we need to compare in a healthy way. Taking reliable result we selected the first way. Each model are trained with same images and each one test their self same test images. We can analyze them more suitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consist of almost six thousands of images. Pre-processing methods that can be applied to images are less than a numerical dataset. We have a few options for pre-processing on images. Those are rotation, shifting, shearing, zooming, rescaling, brightness for each image. We used them separately or a combination of some. Unfortunately, these changes did not raise our prediction results. some combinations decreased accuracy of our results almost half, remaining combinations approached the highest score of the relevant model but never passed. Let's look at which pre-processing how affected our accuracy result.\n",
    "\n",
    "- **Horizontal Flipping:** It did not significantly affect the score. \n",
    "- **Vertical Flipping:** It negatively affects the score. The score decreased by almost %10.\n",
    "- **Zooming:** After 20% zooming, occurring a significant drop in score \n",
    "- **Shifting:** It has 2 options such as right-left and up-down directions. There is no important effect on result\n",
    "- **Rotation:** After 15% rotation, occurring a significant drop in score\n",
    "- **Brightness:** The result gets worse if it is increased too much otherwise it doesn't affect so much. When the brightness was increased, the prediction as pneumonia was increased. When the brightness was increased, darkens are decreasing. About our problem, darkens are important for a healthy lung. The more prominent the lung, the higher you are healthy. That's why the result got worse. \n",
    "- **Rescaling:** It increased the training speed :). We used only this with a 1/255 scaling parameter for each model.\n",
    "- **ZCA Whitening** It's effect not understood\n",
    "\n",
    "These pre-processes only increased our training time. Therefore we didn't use any pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling<a class=\"anchor\" id=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model will be used? Why? What parameters?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 3 different models at this project.\n",
    "- K-nearest Neighbors\n",
    "- Convolutional Neural Network that we designed layers of a model\n",
    "- VGG16. It is a CNN but it is designed by two professors from Oxford for ImageNet-2014 competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor(KNN) Classification\n",
    "\n",
    "The k-Nearest Neighbor classifier is by far the most simple machine learning/image classification algorithm. In fact, it’s so simple that it doesn’t actually “learn” anything.\n",
    "\n",
    "Inside, this algorithm simply relies on the distance between feature vectors. We have the labels associated with each image so we can predict and return an actual category for the image.\n",
    "\n",
    "Simply put, the k-NN algorithm classifies unknown data points by finding the most common class among the k-closest examples. Each data point in the k closest examples normal/pneumonia a vote and the category with the most votes wins!\n",
    "\n",
    "![KNN Visualization](./knn_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the k-nearest Neighbor classification, we need to define a distance metric or similarity function. Common choices include the **Euclidean distance:**\n",
    "$$d(p,q)=\\sqrt{\\sum_{i=1}^{N} (q_i-p_i)^2}$$\n",
    "Other distance metrics/similarity functions can be used depending on your type of data (the chi-squared distance is often used for distributions or manhattan and so on). For this project, we used the Euclidean distance to compare images for similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN at Our Project**\n",
    "\n",
    "First of all we got help from the **scikit learn** library for KNN model generation. To use KNN model, we need to convert the visual information,images, into numerical data. That's why, we encoded the images to feature vectors using **cv2** library. The <em>image_to_feature_vector</em> function do vectorization process.\n",
    "\n",
    "The <em>image_to_feature_vector</em>  method is an extremely naive function that simply takes an input image  and resizes it to a fixed width and height (size ), and then flattens the RGB pixel intensities into a single list of numbers. This means that our input image  will be shrunk to 32 x 32 pixels, and given three channels for each Red, Green, and Blue component respectively, our output “feature vector” will be a list of 32 x 32 x 3 = 3,072 numbers.\n",
    "\n",
    "Our dataset was already splitted hereby we send splitted dataset respectively and saved the extracted features in <em>X/Y_train/test</em> variables.\n",
    "\n",
    "Then we put train vectors to KNN model space and test the model. Selecting the **K** value is easy but we used simpliest way that is brute force. We tried 1-NN to 20-NN and saved the accuracy of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of KNN**\n",
    "\n",
    "It was very surprising. Its accuracy was better than we expect. Actually, we had expected a result close to 60. Our best score is 79,81.\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/Lcn0yF1RcLANG/giphy.gif\" width=\"250\" height=\"250\"/>\n",
    "\n",
    "79,81 is a good result but we thought we can do better than that. Then we decide to design CNN model to improve our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs), or ConvNets, are neural networks that are commonly used for image and audio recognition and classification. CNNs stem from the architecture of an animal brain's visual cortex. Studies have shown that monkey's and cat's visual cortexes have neurons that respond to small subfields of the visual field. Every neuron is responsible for a small section of the visual field, called the receptive field. Together, all neurons in the visual cortex will cover the entire visual space (Hubel, 1968).\n",
    "\n",
    "Like all common neural networks, CNNs have neurons with adjustable weights and biases. Normal neural networks are fully connected, meaning that every single neuron is connected to every neuron from the previous layer. CNNs are not fully connected like normal neural networks, though. This would be too computationally expensive and is simply not needed to achieve the desired results. Using a fully connected neural network would not be very efficient when dealing with image data with large input sizes. To imagine the large number of parameters, think of our chest X-ray images. These images will have an input shape of 64x64x3, or 64 wide, 64 high, with 3 color channels. If a fully connected neural network were to be used, this would mean that a single neuron in a single hidden layer would consist of 12,288 connections (64 x 64 x 3 = 12,288). This is with only one fully connected neuron. Imagine the number of all the weights in a neural network with many neurons! It is easy to understand why fully connected neural networks would not be the most efficient method of classifying images. This is where CNNs come in handy, except a CNN's architecture does in fact include a few fully connected layer(s).\n",
    "\n",
    "Like all neural networks, CNNs have an input and output layer with a number of hidden layers that will apply an activation function, typically ReLu. A CNNs design will consist of three main layers: Convolutional layer, pooling layer, and the fully connected layer. Each layer will be covered below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layer**\n",
    "\n",
    "The convolutional layer is responsible for finding and extractng features from the input data. Convolutional layers use filters, also called kernels, for this feature extracting process. A convolutional layer convolves the input by sliding these filters around the input space while computing the dot product of the weights and inputs. The pixels within the filter will be converted to a single value that will represent the entire receptive field.\n",
    "\n",
    "That in our case we have to decide what activation function we should be utilized in the layers. There are many activation functions such as ReLu, tanh, sigmoid and so on. We used ReLu on in our convolution layers.\n",
    "\n",
    "![](./activation_functions.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling Layer**\n",
    "\n",
    "Pooling layers, otherwise known as downsampling layers, will mostly be seen following convolutional layers of the neural network. The job of the pooling layer is to reduce the spatial dimension of the input. This will result in a reduced number of parameters and will also help our model generalize and avoid overfitting. There are a few types of pooling such as max pooling, average pooling, global pooling. We used maxpooling in this project.\n",
    "\n",
    "![](./Pooling_Simple_max.png)\n",
    "\n",
    "**Max Pooling:** The convolutional layer will find a specific feature in a region within the input and will assign it a higher activation value. The pooling layer will then reduce this region and create a new representation. The max pooling layer essentially creates an abstraction of the original region by using the max values found in each subregion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer**\n",
    "\n",
    "A dense layer is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. The layer has a weight matrix **W**, a bias vector **b**, and the activations of previous layer **a**. The following is te docstring of class Dense from the keras documentation:\n",
    "\n",
    "<em>output = activation(dot(input, kernel) + bias)</em> where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout Layer**\n",
    "\n",
    "Dropout is a a technique used to tackle Overfitting . The Dropout method in keras.layers module takes in a float between 0 and 1, which is the fraction of the neurons to drop. Below is the docstring of the Dropout method from the documentation:\n",
    "\n",
    "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We Designed CNN Layers**\n",
    "\n",
    "|Layer (type)| Output Shape | Param # |\n",
    "|------------|--------------|-----------------|\n",
    "| conv_1 (Conv2D) | (None, 62, 62, 32)         | 896 |\n",
    "| max_pooling2d_1 (MaxPooling2D)| (None, 31, 31, 32)          | 0 |\n",
    "| conv_2 (Conv2D) | (None, 29, 29, 64)         | 18496            |\n",
    "| max_pooling2d_2 (MaxPooling2D)| (None, 14, 14, 64)          | 0             |\n",
    "| flatten_1 (Flatten) | (None, 12544)         | 0            |\n",
    "| dense_1 (Dense)| (None, 128)          | 1605760             |\n",
    "| dropout_1 (Dropout) | (None, 128)         | 3883            |\n",
    "| dense_2 (Dense)| (None, 1)          | 129             |\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|Total params: 1,625,281|\n",
    "|Trainable params: 1,625,281|\n",
    "|Non-trainable params: 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our the best score is %81.70 accuracy from we designed convolutional neural network. We didn't stop to try because the score didn't satisfy us. We think we can get a minimum of 90% accuracy thus we searched the already prepared for image classification models. Then we found the VGG16 and we coded it.\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3o752cDjsI0SB4zdfy/giphy.gif\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used VGG-16 in one of our implementations. VGG-16 is a CNN architecture that used to win an image processing competition in 2014. This architecture is considered one of the best image processing architecture in the time.\n",
    "\n",
    "The input is fixed and it is in 224 x 224 RGB image format. Then the image passes a stack of convolutional layers, where the filters were used with a 3x3 receptive field. This is the smallest size to capture vertical, horizontal and center.\n",
    "\n",
    "Three FC layers follow a stack of convolutional layers. The first two have 4096 channels each, the third performs 100-way ILSVRC classification and contains one for each class total of 1000. The final layer is the soft-max later. The configuration of the connected layers is the same in all networks.\n",
    "\n",
    "All hidden layers are equipped with ReLU non-linearly.\n",
    "The major drawback of this architecture that, it takes a long time to train the data and the big size of the network.\n",
    "We can see the visual representation of the architecture down below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vgg16-visulation.jpg\" height=\"50\" width=\"800\">\n",
    "<img src=\"./vgg16-layers.png\" height=\"50\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we achieved to get higher than 90% accuracy by the VGG16 model.\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/yoJC2GnSClbPOkV0eA/giphy.gif\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation<a class=\"anchor\" id=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adhered to the path we set at proposal report. Firstly, we try to detect pneumonia with K-nearest neighbors method. We got very nice score by knn. We were suprised knn score. It gave us energy to do better. Then we started to design convolutional neural network to prediction sick people from chest X-Ray images. I researched a lot of resources. We designed tiny CNN that consists of only 2 hidden layers. We got better than KNN but we hadn't obtained we wanted score. Therefore we studied hard and we learned new things. There is some prepared model on AI world for competition. We selected one of them. It was VGG16. We examined the layers of VGG16 model. Then we coded it. Finally, we did it, we succeed.\n",
    "\n",
    "| Model |Accuracy Range|\n",
    "|-------|------|\n",
    "|KNN|65-78%|\n",
    "|CNN|75-82%|\n",
    "|VGG16|80-92%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed a lot of things to improve our results. The first of all, we adjusted images for exampled we rotated, zoomed, cutted, changed color/brightness as a result, there was no improvement in the accuracy. Therefore we focused the other training model's parameter. Which parameter are occured on trained model:\n",
    "\n",
    "- **Images Input size:** Bigger size of images takes much more time.\n",
    "- **Epoch Count:** It depends on the model. For example our designed cnn model is overfitting itself after 20 epoch.\n",
    "- **Shuffle:** We splitted our dataset before coding as a result our model was overfitting because the model took 1349 normal chest image and learned only normal chest. Then it took 3883 pheumonia chest and its network was confused. How we understand this? When we looked at our accuracy graph, it changed at every epoch, one epoch good, one epoch bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's discuss the results\n",
    "\n",
    "**KNN Result**\n",
    "\n",
    "![KNN Result](./knnresult.png)\n",
    "\n",
    "To get the best score we should select 2 nearest neighbors. We can get 79.81% accuracy if K value is equal to 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Result**\n",
    "\n",
    "![KNN Result](./cnnAcc.png)\n",
    "![KNN Loss](./cnnLoss.png)\n",
    "\n",
    "When input image size is 64x64, we got 81.70% accuracy at the eighth epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG 16 Result**\n",
    "\n",
    "There were not so many options to change in this architecture. We played along with the different image sizes and epoch numbers. Since this method takes so much time to train the model, we had to use small image size for the reasonable results. (With full image size, the single epoch train time exceeded 1 hour in our workstations)\n",
    "\n",
    "Our best score's graphics is at the below:\n",
    "\n",
    "\n",
    "**Nahanda buraya batu en iyi skorları aldığımız grafikler gelicek. %90 üzeri olanlar**\n",
    "\n",
    "\n",
    "Our best score is XXXXX% accuracy.The parameters:\n",
    "\n",
    "Image Size : XxX\n",
    "Epoch Count: y/X  en iyi acc hangi epoch / toplam kaç epoch yapıldı o modelde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Size 128 x 128**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 128 x 128 images with 15 epochs.It took around 400 minutes to train. In the pre proccessing step, we changed those fields down below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# LOOK AT ME\n",
    "\n",
    "![](./modelvision.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References<a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**\n",
    "https://en.wikipedia.org/wiki/Pneumonia\n",
    "https://www.who.int/news-room/fact-sheets/detail/pneumonia\n",
    "\n",
    "**Dataset**\n",
    "https://data.mendeley.com/datasets/rscbjbr9sj/2 : Dataset's public link\n",
    "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia : Kaggle competition that used the same dataset\n",
    "\n",
    "**Solving Problem Algorithms**\n",
    "https://keras.io/api/ : We usedd Keras Library\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53 : Leaening CNN\n",
    "https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0 : Computer vision for neural networks\n",
    "https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c : Learning VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/26u4lOMA8JKSnL9Uk/giphy.gif\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Disclaimer!** <font color='grey'>This notebook was prepared by <student name(s)> as a term project for the *BBM469 - Data Intensive Applications Laboratory* class. The notebook is available for educational purposes only. There is no guarantee on the correctness of the content provided as it is a student work.\n",
    "\n",
    "If you think there is any copyright violation, please let us [know](https://forms.gle/BNNRB2kR8ZHVEREq8). \n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
